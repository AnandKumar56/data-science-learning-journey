{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1be150ba",
   "metadata": {},
   "source": [
    "# My Journey Learning Google Gemini AI with API Key\n",
    "\n",
    "I'm documenting my learning process as I figure out how to use Google's Gemini AI model through their API. Here's what I'm discovering:\n",
    "\n",
    "1. **Installing the required package** - Learning about pip and package management\n",
    "2. **Setting up my API key** - Understanding authentication with Google's services\n",
    "3. **Creating a model instance** - Building my first AI interaction\n",
    "4. **Common mistakes I made** - And how I fixed them!\n",
    "\n",
    "## What I need before starting:\n",
    "- A Google API key for Gemini AI (I got mine from Google AI Studio)\n",
    "- Basic Python knowledge (still learning as I go!)\n",
    "\n",
    "## My notes while learning:\n",
    "- I need to keep my API key secret - never share it publicly!\n",
    "- The examples here use placeholder keys that won't actually work\n",
    "- I learned about using environment variables for production code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a3cefb-65ee-4b2e-8c1a-6469fb3dc9b4",
   "metadata": {
    "id": "64a3cefb-65ee-4b2e-8c1a-6469fb3dc9b4"
   },
   "source": [
    "## Step 1: Installing the Package - My First Challenge\n",
    "\n",
    "I discovered I need to install a Python package before I can use Google's AI. I found there are two ways to do this:\n",
    "\n",
    "### Method 1: Using pip (I use this in my local environment)\n",
    "### Method 2: Using !pip (I learned this works in Jupyter notebooks)\n",
    "\n",
    "This was confusing at first, but now I understand when to use each method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64a3cefb-65ee-4b2e-8c1a-6469fb3dc9b4",
   "metadata": {
    "id": "64a3cefb-65ee-4b2e-8c1a-6469fb3dc9b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI learns patterns from data to make decisions or predictions, mimicking human-like intelligence.\n"
     ]
    }
   ],
   "source": [
    "# ðŸš€ My Quick Reference - Complete Setup (I figured this out later!)\n",
    "# After struggling with the basics, I created this all-in-one example\n",
    "\n",
    "# What I learned: Import the library first\n",
    "from google import genai\n",
    "\n",
    "# I discovered this advanced method uses environment variables\n",
    "# The client looks for 'GEMINI_API_KEY' in my system environment\n",
    "# Note: This was confusing at first - the simpler method is below!\n",
    "client = genai.Client(api_key='Place your API key here')\n",
    "\n",
    "# My first successful AI question! \n",
    "# I learned that \"model\" specifies which AI brain to use\n",
    "# \"contents\" is where I put my question\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", \n",
    "    contents=\"Explain how AI works in a few words\"\n",
    ")\n",
    "\n",
    "# This is how I get the actual answer (not just metadata)\n",
    "# I made the mistake of forgetting .text many times!\n",
    "print(response.text)\n",
    "\n",
    "# My note: The environment variable method is advanced\n",
    "# I recommend starting with genai.configure() method shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7edd98ca-f7c7-4c8e-ac1c-fe84c1fdb19b",
   "metadata": {
    "id": "7edd98ca-f7c7-4c8e-ac1c-fe84c1fdb19b"
   },
   "outputs": [],
   "source": [
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b404aec-5c12-4879-97a8-8c6caa144493",
   "metadata": {
    "id": "7b404aec-5c12-4879-97a8-8c6caa144493",
    "outputId": "d500b293-b9e0-4f9a-c145-29d6d7ed9146"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-genai in c:\\users\\akn91\\anaconda3\\lib\\site-packages (1.28.0)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from google-genai) (4.9.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from google-genai) (2.40.3)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from google-genai) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from google-genai) (2.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from google-genai) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.3 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from google-genai) (8.2.3)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from google-genai) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from google-genai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.2.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.4.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Method 1: My first attempt using pip\n",
    "# I learned that -U means \"upgrade\" - it installs the latest version\n",
    "# I discovered google-genai is the official package name\n",
    "%pip install -U google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a41ae46c-6bf3-4f76-b30a-860ffbea8114",
   "metadata": {
    "id": "a41ae46c-6bf3-4f76-b30a-860ffbea8114",
    "outputId": "2d1bf17e-8608-4817-a088-de6dbca44bea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-genai in c:\\users\\akn91\\anaconda3\\lib\\site-packages (1.28.0)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from google-genai) (4.9.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from google-genai) (2.40.3)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from google-genai) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from google-genai) (2.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from google-genai) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.3 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from google-genai) (8.2.3)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from google-genai) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from google-genai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.2.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\akn91\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "# Method 2: What I use in Jupyter notebooks\n",
    "# I learned the ! mark tells Jupyter to run system commands\n",
    "# I use this method when working in Google Colab or local Jupyter\n",
    "!pip install -U google-genai\n",
    "\n",
    "# Important lesson I learned the hard way:\n",
    "# After installation, I MUST restart my kernel or I get import errors!\n",
    "# Go to Kernel -> Restart Kernel (I forgot this step so many times!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21d3c2c",
   "metadata": {},
   "source": [
    "## Step 2: My Import Confusion and How I Solved It\n",
    "\n",
    "After installing, I needed to import the package. This is where I got confused at first because I expected to see some output, but nothing happened! I learned this is actually normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57c88ddb-129b-41f6-bd71-fd0cda9ac060",
   "metadata": {
    "id": "57c88ddb-129b-41f6-bd71-fd0cda9ac060"
   },
   "outputs": [],
   "source": [
    "# My confusion: I expected output after importing but got nothing!\n",
    "# What I learned: Python imports are silent when they work correctly\n",
    "# No output actually means SUCCESS!\n",
    "# I only see errors if something goes wrong\n",
    "\n",
    "# Let me test this with a simple example I understand:\n",
    "import random  # I learned this runs silently when it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42ab48c4-f450-4299-be67-dc1a3c1146f8",
   "metadata": {
    "id": "42ab48c4-f450-4299-be67-dc1a3c1146f8"
   },
   "outputs": [],
   "source": [
    "# My test: Import random module (no output = success!)\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "483310f1-f26b-43c3-8afd-a65e6ea41187",
   "metadata": {
    "id": "483310f1-f26b-43c3-8afd-a65e6ea41187",
    "outputId": "7e5a9ec1-950b-4a8a-fb99-931db105d89c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How I verify imports work - by using a function from the module\n",
    "# This generates a random number between 1 and 20\n",
    "import random\n",
    "random.randint(1, 20)  # This DOES show output because it returns a value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb236265-6524-4a64-b8b0-487f3dd2088d",
   "metadata": {
    "id": "fb236265-6524-4a64-b8b0-487f3dd2088d",
    "outputId": "23df3cdc-d080-4fa2-dfe2-f1e6a4441d03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cool trick I learned: importing specific functions\n",
    "# This imports only randint, not the entire random module\n",
    "from random import randint\n",
    "randint(1, 20)  # Now I can use randint directly without \"random.\" prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50f6739f-2416-404a-b74a-302150ff6325",
   "metadata": {
    "id": "50f6739f-2416-404a-b74a-302150ff6325"
   },
   "outputs": [],
   "source": [
    "# Now trying to import the Google AI package\n",
    "# I remember: no output means it worked!\n",
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa1c0fbd-aa50-4d73-93f1-a5dfaf3f1183",
   "metadata": {
    "id": "fa1c0fbd-aa50-4d73-93f1-a5dfaf3f1183",
    "outputId": "fab301b3-c933-4242-e169-5a2535ec2320"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Client',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '_adapters',\n",
       " '_api_client',\n",
       " '_api_module',\n",
       " '_base_url',\n",
       " '_common',\n",
       " '_extra_utils',\n",
       " '_live_converters',\n",
       " '_mcp_utils',\n",
       " '_replay_api_client',\n",
       " '_tokens_converters',\n",
       " '_transformers',\n",
       " 'batches',\n",
       " 'caches',\n",
       " 'chats',\n",
       " 'client',\n",
       " 'errors',\n",
       " 'files',\n",
       " 'live',\n",
       " 'live_music',\n",
       " 'models',\n",
       " 'operations',\n",
       " 'pagers',\n",
       " 'tokens',\n",
       " 'tunings',\n",
       " 'types',\n",
       " 'version']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# My way to verify the import worked - check what's available\n",
    "# dir() shows me all the functions and classes I can use\n",
    "dir(genai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a3de59",
   "metadata": {},
   "source": [
    "## Step 3: I Discovered the Better Import Method\n",
    "\n",
    "While `from google import genai` works, I learned there's a cleaner, more professional way to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be51353f-cb03-42fb-b61b-70e890b27320",
   "metadata": {
    "id": "be51353f-cb03-42fb-b61b-70e890b27320"
   },
   "outputs": [],
   "source": [
    "# MY PREFERRED METHOD: Import with alias\n",
    "# I learned this is the standard, professional way\n",
    "# 'as genai' creates a shorter name so I don't have to type the full name every time\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be51353f-cb03-42fb-b61b-70e890b27320",
   "metadata": {
    "id": "be51353f-cb03-42fb-b61b-70e890b27320"
   },
   "outputs": [],
   "source": [
    "# STEP 4: Setting up my API key (this was tricky!)\n",
    "# I need to replace this placeholder with my real API key\n",
    "# I got my key from: https://aistudio.google.com/app/apikey\n",
    "# Important lesson: NEVER share my real key publicly!\n",
    "\n",
    "# My setup (I replace this with my actual key):\n",
    "genai.configure(api_key=\"Place your API key here\")\n",
    "\n",
    "# Note to self: This is just a placeholder - won't actually work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1faad04e-d5ad-4602-96be-ced52692e974",
   "metadata": {
    "id": "1faad04e-d5ad-4602-96be-ced52692e974",
    "outputId": "c87fc1bb-4eb5-472e-f764-e7e76caa9ddf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(name='models/embedding-gecko-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Embedding Gecko',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=1024,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-1.5-pro-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Pro Latest',\n",
       "       description=('Alias that points to the most recent production (non-experimental) release '\n",
       "                    'of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 '\n",
       "                    'million tokens.'),\n",
       "       input_token_limit=2000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-pro-002',\n",
       "       base_model_id='',\n",
       "       version='002',\n",
       "       display_name='Gemini 1.5 Pro 002',\n",
       "       description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
       "                    'supports up to 2 million tokens, released in September of 2024.'),\n",
       "       input_token_limit=2000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-pro',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Pro',\n",
       "       description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
       "                    'supports up to 2 million tokens, released in May of 2024.'),\n",
       "       input_token_limit=2000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash Latest',\n",
       "       description=('Alias that points to the most recent production (non-experimental) release '\n",
       "                    'of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling '\n",
       "                    'across diverse tasks.'),\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash',\n",
       "       description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n",
       "                    'fast and versatile multimodal model for scaling across diverse tasks.'),\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash-002',\n",
       "       base_model_id='',\n",
       "       version='002',\n",
       "       display_name='Gemini 1.5 Flash 002',\n",
       "       description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
       "                    'for scaling across diverse tasks, released in September of 2024.'),\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash-8b',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash-8B',\n",
       "       description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
       "                    'Flash model, released in October of 2024.'),\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash-8b-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash-8B 001',\n",
       "       description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
       "                    'Flash model, released in October of 2024.'),\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash-8b-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash-8B Latest',\n",
       "       description=('Alias that points to the most recent production (non-experimental) release '\n",
       "                    'of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, '\n",
       "                    'released in October of 2024.'),\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.5-pro-preview-03-25',\n",
       "       base_model_id='',\n",
       "       version='2.5-preview-03-25',\n",
       "       display_name='Gemini 2.5 Pro Preview 03-25',\n",
       "       description='Gemini 2.5 Pro Preview 03-25',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-preview-05-20',\n",
       "       base_model_id='',\n",
       "       version='2.5-preview-05-20',\n",
       "       display_name='Gemini 2.5 Flash Preview 05-20',\n",
       "       description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 2.5 Flash',\n",
       "       description=('Stable version of Gemini 2.5 Flash, our mid-size multimodal model that '\n",
       "                    'supports up to 1 million tokens, released in June of 2025.'),\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-lite-preview-06-17',\n",
       "       base_model_id='',\n",
       "       version='2.5-preview-06-17',\n",
       "       display_name='Gemini 2.5 Flash-Lite Preview 06-17',\n",
       "       description='Preview release (June 11th, 2025) of Gemini 2.5 Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-pro-preview-05-06',\n",
       "       base_model_id='',\n",
       "       version='2.5-preview-05-06',\n",
       "       display_name='Gemini 2.5 Pro Preview 05-06',\n",
       "       description='Preview release (May 6th, 2025) of Gemini 2.5 Pro',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-pro-preview-06-05',\n",
       "       base_model_id='',\n",
       "       version='2.5-preview-06-05',\n",
       "       display_name='Gemini 2.5 Pro Preview',\n",
       "       description='Preview release (June 5th, 2025) of Gemini 2.5 Pro',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-pro',\n",
       "       base_model_id='',\n",
       "       version='2.5',\n",
       "       display_name='Gemini 2.5 Pro',\n",
       "       description='Stable release (June 17th, 2025) of Gemini 2.5 Pro',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.0-flash-exp',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash Experimental',\n",
       "       description='Gemini 2.0 Flash Experimental',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash',\n",
       "       description='Gemini 2.0 Flash',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-001',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash 001',\n",
       "       description=('Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model '\n",
       "                    'for scaling across diverse tasks, released in January of 2025.'),\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-exp-image-generation',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash (Image Generation) Experimental',\n",
       "       description='Gemini 2.0 Flash (Image Generation) Experimental',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-lite-001',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash-Lite 001',\n",
       "       description='Stable version of Gemini 2.0 Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-lite',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash-Lite',\n",
       "       description='Gemini 2.0 Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-preview-image-generation',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash Preview Image Generation',\n",
       "       description='Gemini 2.0 Flash Preview Image Generation',\n",
       "       input_token_limit=32768,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.0-flash-lite-preview-02-05',\n",
       "       base_model_id='',\n",
       "       version='preview-02-05',\n",
       "       display_name='Gemini 2.0 Flash-Lite Preview 02-05',\n",
       "       description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-lite-preview',\n",
       "       base_model_id='',\n",
       "       version='preview-02-05',\n",
       "       display_name='Gemini 2.0 Flash-Lite Preview',\n",
       "       description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-pro-exp',\n",
       "       base_model_id='',\n",
       "       version='2.5-exp-03-25',\n",
       "       display_name='Gemini 2.0 Pro Experimental',\n",
       "       description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.0-pro-exp-02-05',\n",
       "       base_model_id='',\n",
       "       version='2.5-exp-03-25',\n",
       "       display_name='Gemini 2.0 Pro Experimental 02-05',\n",
       "       description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-exp-1206',\n",
       "       base_model_id='',\n",
       "       version='2.5-exp-03-25',\n",
       "       display_name='Gemini Experimental 1206',\n",
       "       description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.0-flash-thinking-exp-01-21',\n",
       "       base_model_id='',\n",
       "       version='2.5-preview-05-20',\n",
       "       display_name='Gemini 2.5 Flash Preview 05-20',\n",
       "       description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.0-flash-thinking-exp',\n",
       "       base_model_id='',\n",
       "       version='2.5-preview-05-20',\n",
       "       display_name='Gemini 2.5 Flash Preview 05-20',\n",
       "       description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.0-flash-thinking-exp-1219',\n",
       "       base_model_id='',\n",
       "       version='2.5-preview-05-20',\n",
       "       display_name='Gemini 2.5 Flash Preview 05-20',\n",
       "       description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-preview-tts',\n",
       "       base_model_id='',\n",
       "       version='gemini-2.5-flash-exp-tts-2025-05-19',\n",
       "       display_name='Gemini 2.5 Flash Preview TTS',\n",
       "       description='Gemini 2.5 Flash Preview TTS',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=16384,\n",
       "       supported_generation_methods=['countTokens', 'generateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-pro-preview-tts',\n",
       "       base_model_id='',\n",
       "       version='gemini-2.5-pro-preview-tts-2025-05-19',\n",
       "       display_name='Gemini 2.5 Pro Preview TTS',\n",
       "       description='Gemini 2.5 Pro Preview TTS',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=16384,\n",
       "       supported_generation_methods=['countTokens', 'generateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/learnlm-2.0-flash-experimental',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='LearnLM 2.0 Flash Experimental',\n",
       "       description='LearnLM 2.0 Flash Experimental',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=32768,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3-1b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3 1B',\n",
       "       description='',\n",
       "       input_token_limit=32768,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3-4b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3 4B',\n",
       "       description='',\n",
       "       input_token_limit=32768,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3-12b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3 12B',\n",
       "       description='',\n",
       "       input_token_limit=32768,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3-27b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3 27B',\n",
       "       description='',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3n-e4b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3n E4B',\n",
       "       description='',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3n-e2b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3n E2B',\n",
       "       description='',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-lite',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 2.5 Flash-Lite',\n",
       "       description='Stable verion of Gemini 2.5 Flash-Lite, released in July of 2025',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/embedding-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Embedding 001',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=2048,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/text-embedding-004',\n",
       "       base_model_id='',\n",
       "       version='004',\n",
       "       display_name='Text Embedding 004',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=2048,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-embedding-exp-03-07',\n",
       "       base_model_id='',\n",
       "       version='exp-03-07',\n",
       "       display_name='Gemini Embedding Experimental 03-07',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-embedding-exp',\n",
       "       base_model_id='',\n",
       "       version='exp-03-07',\n",
       "       display_name='Gemini Embedding Experimental',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-embedding-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini Embedding 001',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=2048,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/aqa',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Model that performs Attributed Question Answering.',\n",
       "       description=('Model trained to return answers to questions that are grounded in provided '\n",
       "                    'sources, along with estimating answerable probability.'),\n",
       "       input_token_limit=7168,\n",
       "       output_token_limit=1024,\n",
       "       supported_generation_methods=['generateAnswer'],\n",
       "       temperature=0.2,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=40),\n",
       " Model(name='models/imagen-3.0-generate-002',\n",
       "       base_model_id='',\n",
       "       version='002',\n",
       "       display_name='Imagen 3.0 002 model',\n",
       "       description='Vertex served Imagen 3.0 002 model',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predict'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/imagen-4.0-generate-preview-06-06',\n",
       "       base_model_id='',\n",
       "       version='01',\n",
       "       display_name='Imagen 4 (Preview)',\n",
       "       description='Vertex served Imagen 4.0 model',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predict'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/imagen-4.0-ultra-generate-preview-06-06',\n",
       "       base_model_id='',\n",
       "       version='01',\n",
       "       display_name='Imagen 4 Ultra (Preview)',\n",
       "       description='Vertex served Imagen 4.0 ultra model',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predict'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/veo-2.0-generate-001',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Veo 2',\n",
       "       description=('Vertex served Veo 2 model. Access to this model requires billing to be '\n",
       "                    'enabled on the associated Google Cloud Platform account. Please visit '\n",
       "                    'https://console.cloud.google.com/billing to enable it.'),\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predictLongRunning'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/veo-3.0-generate-preview',\n",
       "       base_model_id='',\n",
       "       version='3.0',\n",
       "       display_name='Veo 3',\n",
       "       description=('Veo 3 preview. Access to this model requires billing to be enabled on the '\n",
       "                    'associated Google Cloud Platform account. Please visit '\n",
       "                    'https://console.cloud.google.com/billing to enable it.'),\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predictLongRunning'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/veo-3.0-fast-generate-preview',\n",
       "       base_model_id='',\n",
       "       version='3.0',\n",
       "       display_name='Veo 3 fast',\n",
       "       description=('Veo 3 fast preview. Access to this model requires billing to be enabled on '\n",
       "                    'the associated Google Cloud Platform account. Please visit '\n",
       "                    'https://console.cloud.google.com/billing to enable it.'),\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predictLongRunning'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-2.5-flash-preview-native-audio-dialog',\n",
       "       base_model_id='',\n",
       "       version='gemini-2.5-flash-preview-native-audio-dialog-2025-05-19',\n",
       "       display_name='Gemini 2.5 Flash Preview Native Audio Dialog',\n",
       "       description='Gemini 2.5 Flash Preview Native Audio Dialog',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-exp-native-audio-thinking-dialog',\n",
       "       base_model_id='',\n",
       "       version='gemini-2.5-flash-exp-native-audio-thinking-dialog-2025-05-19',\n",
       "       display_name='Gemini 2.5 Flash Exp Native Audio Thinking Dialog',\n",
       "       description='Gemini 2.5 Flash Exp Native Audio Thinking Dialog',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.0-flash-live-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 2.0 Flash 001',\n",
       "       description='Gemini 2.0 Flash 001',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-live-2.5-flash-preview',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini Live 2.5 Flash Preview',\n",
       "       description='Gemini Live 2.5 Flash Preview',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-live-preview',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 2.5 Flash Live Preview',\n",
       "       description='Gemini 2.5 Flash Live Preview',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 5: List available models\n",
    "# This shows all Gemini models you can use with your API key\n",
    "models = genai.list_models()\n",
    "\n",
    "# Convert the models object to a list so we can see the available models\n",
    "# list() converts other data types to a list format\n",
    "list(models)  # This will show all available Gemini models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1faad04e-d5ad-4602-96be-ced52692e974",
   "metadata": {
    "id": "1faad04e-d5ad-4602-96be-ced52692e974",
    "outputId": "c87fc1bb-4eb5-472e-f764-e7e76caa9ddf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(name='models/embedding-gecko-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Embedding Gecko',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=1024,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-1.5-pro-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Pro Latest',\n",
       "       description=('Alias that points to the most recent production (non-experimental) release '\n",
       "                    'of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 '\n",
       "                    'million tokens.'),\n",
       "       input_token_limit=2000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-pro-002',\n",
       "       base_model_id='',\n",
       "       version='002',\n",
       "       display_name='Gemini 1.5 Pro 002',\n",
       "       description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
       "                    'supports up to 2 million tokens, released in September of 2024.'),\n",
       "       input_token_limit=2000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-pro',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Pro',\n",
       "       description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
       "                    'supports up to 2 million tokens, released in May of 2024.'),\n",
       "       input_token_limit=2000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash Latest',\n",
       "       description=('Alias that points to the most recent production (non-experimental) release '\n",
       "                    'of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling '\n",
       "                    'across diverse tasks.'),\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash',\n",
       "       description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n",
       "                    'fast and versatile multimodal model for scaling across diverse tasks.'),\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash-002',\n",
       "       base_model_id='',\n",
       "       version='002',\n",
       "       display_name='Gemini 1.5 Flash 002',\n",
       "       description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
       "                    'for scaling across diverse tasks, released in September of 2024.'),\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash-8b',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash-8B',\n",
       "       description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
       "                    'Flash model, released in October of 2024.'),\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash-8b-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash-8B 001',\n",
       "       description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
       "                    'Flash model, released in October of 2024.'),\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash-8b-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash-8B Latest',\n",
       "       description=('Alias that points to the most recent production (non-experimental) release '\n",
       "                    'of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, '\n",
       "                    'released in October of 2024.'),\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.5-pro-preview-03-25',\n",
       "       base_model_id='',\n",
       "       version='2.5-preview-03-25',\n",
       "       display_name='Gemini 2.5 Pro Preview 03-25',\n",
       "       description='Gemini 2.5 Pro Preview 03-25',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-preview-05-20',\n",
       "       base_model_id='',\n",
       "       version='2.5-preview-05-20',\n",
       "       display_name='Gemini 2.5 Flash Preview 05-20',\n",
       "       description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 2.5 Flash',\n",
       "       description=('Stable version of Gemini 2.5 Flash, our mid-size multimodal model that '\n",
       "                    'supports up to 1 million tokens, released in June of 2025.'),\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-lite-preview-06-17',\n",
       "       base_model_id='',\n",
       "       version='2.5-preview-06-17',\n",
       "       display_name='Gemini 2.5 Flash-Lite Preview 06-17',\n",
       "       description='Preview release (June 11th, 2025) of Gemini 2.5 Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-pro-preview-05-06',\n",
       "       base_model_id='',\n",
       "       version='2.5-preview-05-06',\n",
       "       display_name='Gemini 2.5 Pro Preview 05-06',\n",
       "       description='Preview release (May 6th, 2025) of Gemini 2.5 Pro',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-pro-preview-06-05',\n",
       "       base_model_id='',\n",
       "       version='2.5-preview-06-05',\n",
       "       display_name='Gemini 2.5 Pro Preview',\n",
       "       description='Preview release (June 5th, 2025) of Gemini 2.5 Pro',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-pro',\n",
       "       base_model_id='',\n",
       "       version='2.5',\n",
       "       display_name='Gemini 2.5 Pro',\n",
       "       description='Stable release (June 17th, 2025) of Gemini 2.5 Pro',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.0-flash-exp',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash Experimental',\n",
       "       description='Gemini 2.0 Flash Experimental',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash',\n",
       "       description='Gemini 2.0 Flash',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-001',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash 001',\n",
       "       description=('Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model '\n",
       "                    'for scaling across diverse tasks, released in January of 2025.'),\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-exp-image-generation',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash (Image Generation) Experimental',\n",
       "       description='Gemini 2.0 Flash (Image Generation) Experimental',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-lite-001',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash-Lite 001',\n",
       "       description='Stable version of Gemini 2.0 Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-lite',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash-Lite',\n",
       "       description='Gemini 2.0 Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-preview-image-generation',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash Preview Image Generation',\n",
       "       description='Gemini 2.0 Flash Preview Image Generation',\n",
       "       input_token_limit=32768,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.0-flash-lite-preview-02-05',\n",
       "       base_model_id='',\n",
       "       version='preview-02-05',\n",
       "       display_name='Gemini 2.0 Flash-Lite Preview 02-05',\n",
       "       description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-lite-preview',\n",
       "       base_model_id='',\n",
       "       version='preview-02-05',\n",
       "       display_name='Gemini 2.0 Flash-Lite Preview',\n",
       "       description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-pro-exp',\n",
       "       base_model_id='',\n",
       "       version='2.5-exp-03-25',\n",
       "       display_name='Gemini 2.0 Pro Experimental',\n",
       "       description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.0-pro-exp-02-05',\n",
       "       base_model_id='',\n",
       "       version='2.5-exp-03-25',\n",
       "       display_name='Gemini 2.0 Pro Experimental 02-05',\n",
       "       description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-exp-1206',\n",
       "       base_model_id='',\n",
       "       version='2.5-exp-03-25',\n",
       "       display_name='Gemini Experimental 1206',\n",
       "       description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.0-flash-thinking-exp-01-21',\n",
       "       base_model_id='',\n",
       "       version='2.5-preview-05-20',\n",
       "       display_name='Gemini 2.5 Flash Preview 05-20',\n",
       "       description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.0-flash-thinking-exp',\n",
       "       base_model_id='',\n",
       "       version='2.5-preview-05-20',\n",
       "       display_name='Gemini 2.5 Flash Preview 05-20',\n",
       "       description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.0-flash-thinking-exp-1219',\n",
       "       base_model_id='',\n",
       "       version='2.5-preview-05-20',\n",
       "       display_name='Gemini 2.5 Flash Preview 05-20',\n",
       "       description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-preview-tts',\n",
       "       base_model_id='',\n",
       "       version='gemini-2.5-flash-exp-tts-2025-05-19',\n",
       "       display_name='Gemini 2.5 Flash Preview TTS',\n",
       "       description='Gemini 2.5 Flash Preview TTS',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=16384,\n",
       "       supported_generation_methods=['countTokens', 'generateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-pro-preview-tts',\n",
       "       base_model_id='',\n",
       "       version='gemini-2.5-pro-preview-tts-2025-05-19',\n",
       "       display_name='Gemini 2.5 Pro Preview TTS',\n",
       "       description='Gemini 2.5 Pro Preview TTS',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=16384,\n",
       "       supported_generation_methods=['countTokens', 'generateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/learnlm-2.0-flash-experimental',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='LearnLM 2.0 Flash Experimental',\n",
       "       description='LearnLM 2.0 Flash Experimental',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=32768,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3-1b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3 1B',\n",
       "       description='',\n",
       "       input_token_limit=32768,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3-4b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3 4B',\n",
       "       description='',\n",
       "       input_token_limit=32768,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3-12b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3 12B',\n",
       "       description='',\n",
       "       input_token_limit=32768,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3-27b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3 27B',\n",
       "       description='',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3n-e4b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3n E4B',\n",
       "       description='',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3n-e2b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3n E2B',\n",
       "       description='',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-lite',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 2.5 Flash-Lite',\n",
       "       description='Stable verion of Gemini 2.5 Flash-Lite, released in July of 2025',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/embedding-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Embedding 001',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=2048,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/text-embedding-004',\n",
       "       base_model_id='',\n",
       "       version='004',\n",
       "       display_name='Text Embedding 004',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=2048,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-embedding-exp-03-07',\n",
       "       base_model_id='',\n",
       "       version='exp-03-07',\n",
       "       display_name='Gemini Embedding Experimental 03-07',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-embedding-exp',\n",
       "       base_model_id='',\n",
       "       version='exp-03-07',\n",
       "       display_name='Gemini Embedding Experimental',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-embedding-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini Embedding 001',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=2048,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/aqa',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Model that performs Attributed Question Answering.',\n",
       "       description=('Model trained to return answers to questions that are grounded in provided '\n",
       "                    'sources, along with estimating answerable probability.'),\n",
       "       input_token_limit=7168,\n",
       "       output_token_limit=1024,\n",
       "       supported_generation_methods=['generateAnswer'],\n",
       "       temperature=0.2,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=40),\n",
       " Model(name='models/imagen-3.0-generate-002',\n",
       "       base_model_id='',\n",
       "       version='002',\n",
       "       display_name='Imagen 3.0 002 model',\n",
       "       description='Vertex served Imagen 3.0 002 model',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predict'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/imagen-4.0-generate-preview-06-06',\n",
       "       base_model_id='',\n",
       "       version='01',\n",
       "       display_name='Imagen 4 (Preview)',\n",
       "       description='Vertex served Imagen 4.0 model',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predict'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/imagen-4.0-ultra-generate-preview-06-06',\n",
       "       base_model_id='',\n",
       "       version='01',\n",
       "       display_name='Imagen 4 Ultra (Preview)',\n",
       "       description='Vertex served Imagen 4.0 ultra model',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predict'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/veo-2.0-generate-001',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Veo 2',\n",
       "       description=('Vertex served Veo 2 model. Access to this model requires billing to be '\n",
       "                    'enabled on the associated Google Cloud Platform account. Please visit '\n",
       "                    'https://console.cloud.google.com/billing to enable it.'),\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predictLongRunning'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/veo-3.0-generate-preview',\n",
       "       base_model_id='',\n",
       "       version='3.0',\n",
       "       display_name='Veo 3',\n",
       "       description=('Veo 3 preview. Access to this model requires billing to be enabled on the '\n",
       "                    'associated Google Cloud Platform account. Please visit '\n",
       "                    'https://console.cloud.google.com/billing to enable it.'),\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predictLongRunning'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/veo-3.0-fast-generate-preview',\n",
       "       base_model_id='',\n",
       "       version='3.0',\n",
       "       display_name='Veo 3 fast',\n",
       "       description=('Veo 3 fast preview. Access to this model requires billing to be enabled on '\n",
       "                    'the associated Google Cloud Platform account. Please visit '\n",
       "                    'https://console.cloud.google.com/billing to enable it.'),\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predictLongRunning'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-2.5-flash-preview-native-audio-dialog',\n",
       "       base_model_id='',\n",
       "       version='gemini-2.5-flash-preview-native-audio-dialog-2025-05-19',\n",
       "       display_name='Gemini 2.5 Flash Preview Native Audio Dialog',\n",
       "       description='Gemini 2.5 Flash Preview Native Audio Dialog',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-exp-native-audio-thinking-dialog',\n",
       "       base_model_id='',\n",
       "       version='gemini-2.5-flash-exp-native-audio-thinking-dialog-2025-05-19',\n",
       "       display_name='Gemini 2.5 Flash Exp Native Audio Thinking Dialog',\n",
       "       description='Gemini 2.5 Flash Exp Native Audio Thinking Dialog',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.0-flash-live-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 2.0 Flash 001',\n",
       "       description='Gemini 2.0 Flash 001',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-live-2.5-flash-preview',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini Live 2.5 Flash Preview',\n",
       "       description='Gemini Live 2.5 Flash Preview',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-live-preview',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 2.5 Flash Live Preview',\n",
       "       description='Gemini 2.5 Flash Live Preview',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMPLETE SETUP IN ONE CELL (Alternative approach)\n",
    "# This cell combines all setup steps in one place for convenience\n",
    "\n",
    "# Step 1: Import the package\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Step 2: Configure API key (replace with your actual key)\n",
    "genai.configure(api_key=\"Place your API key here\")\n",
    "\n",
    "# Step 3: List available models\n",
    "models = genai.list_models()\n",
    "list(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf14479",
   "metadata": {},
   "source": [
    "## Step 6: Create a Model Instance and Generate Content\n",
    "\n",
    "Now that we have our API configured, let's create a model instance and generate some AI responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6a47492-5769-4275-9767-090f1dde1442",
   "metadata": {
    "id": "e6a47492-5769-4275-9767-090f1dde1442",
    "outputId": "ce0d6407-41a7-4153-8175-0a2935509c15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genai.GenerativeModel(\n",
       "    model_name='models/gemini-2.5-flash',\n",
       "    generation_config={},\n",
       "    safety_settings={},\n",
       "    tools=None,\n",
       "    system_instruction=None,\n",
       "    cached_content=None\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a model instance using Gemini 2.5 Flash (fastest model)\n",
    "# GenerativeModel() creates an instance of the AI model we can interact with\n",
    "# 'gemini-2.5-flash' is the model name - it's fast and efficient for most tasks\n",
    "model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "\n",
    "# Display the model object (this shows it was created successfully)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c49a28b-4451-4447-b8df-296d40a185e0",
   "metadata": {
    "id": "3c49a28b-4451-4447-b8df-296d40a185e0",
    "outputId": "dd1d73e5-fae1-4096-e2a1-dd6c0287b64c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Identifying the \\\"top 5\\\" cricketers in India is always a bit subjective, as it can depend on current form, format preference (Test, ODI, T20), and overall career impact. However, based on recent performances, impact across formats, and standing in the game, a strong case can be made for these players:\\n\\n1.  **Virat Kohli:** Still the batting lynchpin across formats, a prolific run-scorer, and widely regarded as one of the greatest batsmen of all time. His ability to anchor innings and chase targets remains unparalleled.\\n\\n2.  **Rohit Sharma:** The captain of the Indian team (across formats), a world-class opener, especially in white-ball cricket, known for his elegant stroke play and ability to score big hundreds. His leadership and big-match temperament are key.\\n\\n3.  **Jasprit Bumrah:** Widely regarded as one of the best fast bowlers in the world, adept at all formats. His unique action, pace, accuracy, and death bowling skills make him invaluable and a genuine match-winner.\\n\\n4.  **Ravindra Jadeja:** A true all-rounder who excels in all three departments. A reliable left-arm spinner, an attacking lower-order batsman, and arguably one of the best fielders in the world. His presence provides immense balance and depth to the team.\\n\\n5.  **Hardik Pandya:** When fit and firing, he's an absolute game-changer with his explosive batting and handy medium pace. His all-round abilities are crucial, especially in white-ball cricket, where he provides a rare balance to the playing XI.\\n\\n**Close Contenders/Strong Arguments for the 5th Spot (or even higher, depending on recent form/format):**\\n\\n*   **Mohammed Shami:** An outstanding fast bowler, particularly lethal in Test and ODI formats. His recent form, especially in the World Cup, showcased his incredible wicket-taking prowess.\\n*   **Rishabh Pant:** While currently recovering from injury, his match-winning abilities, especially in Test cricket, are undeniable. His aggressive batting style can turn games on its head quickly.\\n*   **Ravichandran Ashwin:** A modern-day spin legend in Test cricket, known for his variations and tactical brilliance. While less prominent in white-ball, his Test record is phenomenal and he's still a vital cog in the Test team.\\n\\nThe top 4 listed are fairly consistent choices, with the 5th spot often debated among the talented pool of players based on their current impact and specific format strengths.\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 10,\n",
       "        \"candidates_token_count\": 530,\n",
       "        \"total_token_count\": 2114\n",
       "      },\n",
       "      \"model_version\": \"gemini-2.5-flash\"\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate content using the model\n",
    "# generate_content() sends your prompt to the AI and gets a response\n",
    "# The text in quotes is your prompt/question to the AI\n",
    "response = model.generate_content('who are the top 5 cricketers in India')\n",
    "\n",
    "# Display the response object (contains the AI's response and metadata)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c49a28b-4451-4447-b8df-296d40a185e0",
   "metadata": {
    "id": "3c49a28b-4451-4447-b8df-296d40a185e0",
    "outputId": "dd1d73e5-fae1-4096-e2a1-dd6c0287b64c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Identifying the \\\"top 5\\\" cricketers in India can be subjective and depends on whether you're considering current form, all-time impact, or multi-format performance.\\n\\nHowever, based on current influence, consistent performance across formats (or dominance in their specialized format), and match-winning ability, here's a widely accepted list of 5 prominent Indian cricketers:\\n\\n1.  **Virat Kohli:** Often considered one of the greatest batsmen of all time, Kohli is a prolific run-scorer across all formats. His consistency, ability to chase targets, and leadership qualities (as former captain) make him an undisputed top player.\\n\\n2.  **Rohit Sharma:** The current captain of the Indian team in all formats, Rohit is known for his elegant strokeplay, ability to score big hundreds (especially in ODIs), and calm leadership. He's one of the most destructive openers in white-ball cricket.\\n\\n3.  **Jasprit Bumrah:** A world-class fast bowler, Bumrah is unique with his unconventional action, pinpoint yorkers, and variations. He's a genuine match-winner in all three formats and is arguably one of the best pacers globally when fit.\\n\\n4.  **Ravindra Jadeja:** An exceptional all-rounder, Jadeja is a left-arm spinner who can pick crucial wickets, a dynamic fielder, and a handy lower-order batsman. His three-dimensional skills make him invaluable to the team across formats.\\n\\n5.  **Shubman Gill:** One of the most exciting young talents, Gill has rapidly established himself as a multi-format opener. With elegant strokeplay and a strong temperament, he's shown remarkable consistency and is widely tipped to be a future superstar for India.\\n\\n**Honourable Mentions:**\\n\\n*   **Mohammed Shami:** A fantastic fast bowler, especially in Tests and ODIs, known for his seam movement and wicket-taking ability.\\n*   **Rishabh Pant:** An explosive wicketkeeper-batsman, a Test match-winner, though currently recovering from injury.\\n*   **Ravichandran Ashwin:** A modern-day Test legend with numerous records and variations in his off-spin bowling.\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 10,\n",
       "        \"candidates_token_count\": 451,\n",
       "        \"total_token_count\": 1486\n",
       "      },\n",
       "      \"model_version\": \"gemini-2.5-flash\"\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMPLETE WORKING EXAMPLE (with a different API key for testing)\n",
    "# This cell shows the complete process from start to finish\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configure with API key (this is a different example key)\n",
    "genai.configure(api_key=\"Place your API key here\")\n",
    "\n",
    "# Create model instance\n",
    "model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "\n",
    "# Generate content\n",
    "response = model.generate_content('who are the top 5 cricketers in India')\n",
    "\n",
    "# Display the response object\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3897fe8a-e74a-4541-ac74-4e28ec43dc12",
   "metadata": {
    "id": "3897fe8a-e74a-4541-ac74-4e28ec43dc12",
    "outputId": "69039e82-7ad9-483d-839e-e4a24e6be139"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying the \"top 5\" cricketers in India is always a bit subjective, as it can depend on current form, format preference (Test, ODI, T20), and overall career impact. However, based on recent performances, impact across formats, and standing in the game, a strong case can be made for these players:\n",
      "\n",
      "1.  **Virat Kohli:** Still the batting lynchpin across formats, a prolific run-scorer, and widely regarded as one of the greatest batsmen of all time. His ability to anchor innings and chase targets remains unparalleled.\n",
      "\n",
      "2.  **Rohit Sharma:** The captain of the Indian team (across formats), a world-class opener, especially in white-ball cricket, known for his elegant stroke play and ability to score big hundreds. His leadership and big-match temperament are key.\n",
      "\n",
      "3.  **Jasprit Bumrah:** Widely regarded as one of the best fast bowlers in the world, adept at all formats. His unique action, pace, accuracy, and death bowling skills make him invaluable and a genuine match-winner.\n",
      "\n",
      "4.  **Ravindra Jadeja:** A true all-rounder who excels in all three departments. A reliable left-arm spinner, an attacking lower-order batsman, and arguably one of the best fielders in the world. His presence provides immense balance and depth to the team.\n",
      "\n",
      "5.  **Hardik Pandya:** When fit and firing, he's an absolute game-changer with his explosive batting and handy medium pace. His all-round abilities are crucial, especially in white-ball cricket, where he provides a rare balance to the playing XI.\n",
      "\n",
      "**Close Contenders/Strong Arguments for the 5th Spot (or even higher, depending on recent form/format):**\n",
      "\n",
      "*   **Mohammed Shami:** An outstanding fast bowler, particularly lethal in Test and ODI formats. His recent form, especially in the World Cup, showcased his incredible wicket-taking prowess.\n",
      "*   **Rishabh Pant:** While currently recovering from injury, his match-winning abilities, especially in Test cricket, are undeniable. His aggressive batting style can turn games on its head quickly.\n",
      "*   **Ravichandran Ashwin:** A modern-day spin legend in Test cricket, known for his variations and tactical brilliance. While less prominent in white-ball, his Test record is phenomenal and he's still a vital cog in the Test team.\n",
      "\n",
      "The top 4 listed are fairly consistent choices, with the 5th spot often debated among the talented pool of players based on their current impact and specific format strengths.\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: Extract the actual text from the response\n",
    "# The response object contains metadata, but we want just the text\n",
    "# Use .text to get the actual AI-generated text content\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3897fe8a-e74a-4541-ac74-4e28ec43dc12",
   "metadata": {
    "id": "3897fe8a-e74a-4541-ac74-4e28ec43dc12",
    "outputId": "69039e82-7ad9-483d-839e-e4a24e6be139"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying the \"top 5\" cricketers in India is always a subjective exercise, as it depends on factors like current form, consistency, impact across formats, and overall contribution to the team. However, based on recent performances, consistent excellence, and their importance to the Indian team, here are five players who are widely considered among the best:\n",
      "\n",
      "1.  **Virat Kohli:** A modern-day legend, Kohli is one of the most prolific run-scorers in the history of the game. His consistency, ability to score centuries, and master of the chase make him an unparalleled force, especially in ODIs and T20Is, while still being a formidable Test batter.\n",
      "\n",
      "2.  **Rohit Sharma:** The current captain of the Indian team across all formats, Rohit is known for his elegant stroke play, ability to hit massive sixes, and his exceptional record as an opener, especially in white-ball cricket where he has multiple double centuries in ODIs and four T20I hundreds. His leadership is also a significant factor.\n",
      "\n",
      "3.  **Jasprit Bumrah:** Widely regarded as one of the best fast bowlers in the world across all formats, Bumrah's unique action, pinpoint accuracy, lethal yorkers, and ability to generate pace and bounce make him incredibly difficult to face. He's a genuine wicket-taker and death-overs specialist.\n",
      "\n",
      "4.  **Ravindra Jadeja:** A true multi-format all-rounder, Jadeja brings immense value with his left-arm spin, explosive batting, and world-class fielding. He's a vital cog in the team, capable of breaking partnerships, scoring crucial runs down the order, and pulling off spectacular catches and run-outs.\n",
      "\n",
      "5.  **Hardik Pandya:** An explosive all-rounder, Hardik is a game-changer with his powerful hitting, medium-fast bowling, and athletic fielding. He provides crucial balance to the white-ball teams, often providing late impetus with the bat and picking up key wickets. He has also successfully led the T20I side.\n",
      "\n",
      "**Honorable Mentions:**\n",
      "*   **Mohammed Shami:** A fantastic pace bowler, especially in white-ball cricket, known for his seam movement and wicket-taking ability.\n",
      "*   **Rishabh Pant:** An incredibly exciting and match-winning wicketkeeper-batter, especially in Test cricket, though currently recovering from injury.\n",
      "*   **Shubman Gill:** A rising star who has shown immense promise and consistent run-scoring across formats.\n"
     ]
    }
   ],
   "source": [
    "# FINAL COMPLETE EXAMPLE: Everything together with proper output\n",
    "# This is the clean, final version that shows the AI's text response\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configure API key\n",
    "genai.configure(api_key=\"Place your API key here\")\n",
    "\n",
    "# Create model\n",
    "model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "\n",
    "# Generate content\n",
    "response = model.generate_content('who are the top 5 cricketers in India')\n",
    "\n",
    "# Print the actual text response (this is what you want to see!)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e7fa3a",
   "metadata": {},
   "source": [
    "## My Mistakes and How I Fixed Them\n",
    "\n",
    "I made a lot of errors while learning this! Here are the mistakes I made and how I figured out the solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdcabe92-f273-4b57-85e8-10b5cfefabcf",
   "metadata": {
    "id": "fdcabe92-f273-4b57-85e8-10b5cfefabcf"
   },
   "outputs": [],
   "source": [
    "# MY MISTAKE #1: \"Why don't I see any output after importing?\"\n",
    "# WHAT I LEARNED: This is actually NORMAL! Python imports are silent when successful\n",
    "# \n",
    "# I used to think no output meant something was broken\n",
    "# Now I know: no output means the import WORKED!\n",
    "# I only see messages when there are errors\n",
    "\n",
    "import google.generativeai as genai\n",
    "# â†‘ I learned this shows no output - and that's perfect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1b0e8d4-6098-403a-a216-0c85e8f821c3",
   "metadata": {
    "id": "d1b0e8d4-6098-403a-a216-0c85e8f821c3"
   },
   "outputs": [],
   "source": [
    "# ERROR #2: \"My calculations don't show results\"\n",
    "# SOLUTION: In Jupyter notebooks, the last line of a cell is automatically displayed\n",
    "# \n",
    "# Example - these variables are created but not displayed:\n",
    "a = 100\n",
    "b = 200\n",
    "# To see the result, either use print() or put the operation on the last line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0b8234c-7497-4851-957d-0bdf3ffbbec6",
   "metadata": {
    "id": "e0b8234c-7497-4851-957d-0bdf3ffbbec6",
    "outputId": "a1cbe1c6-610a-432c-eef9-f6df5a740739"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will show the result because it's the last line in the cell\n",
    "# Jupyter automatically displays the result of the last expression\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f53dba3-8ddd-430f-a858-da9acbef2a20",
   "metadata": {
    "id": "1f53dba3-8ddd-430f-a858-da9acbef2a20"
   },
   "outputs": [],
   "source": [
    "# ERROR #3: \"Package not found\" or \"Module not found\"\n",
    "# SOLUTION: After installing a package, you MUST restart your kernel\n",
    "#\n",
    "# Steps to fix:\n",
    "# 1. Install the package: !pip install -U google-genai\n",
    "# 2. Go to Kernel â†’ Restart Kernel (or Restart & Run All)\n",
    "# 3. Run your import statement again\n",
    "#\n",
    "# Many beginners skip the restart step and get confused!\n",
    "\n",
    "import google.generativeai as genai\n",
    "# â†‘ Run this AFTER restarting kernel if you get import errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "314301ff-670d-469f-a9b3-0510afcc5629",
   "metadata": {
    "id": "314301ff-670d-469f-a9b3-0510afcc5629"
   },
   "outputs": [],
   "source": [
    "# ERROR #4: API key format errors\n",
    "# WRONG: Including <> brackets in your API key\n",
    "# genai.configure(api_key=\"<YOUR_API_KEY_HERE>\")  # âŒ Don't include < >\n",
    "\n",
    "# CORRECT: Use your actual API key without any brackets\n",
    "# genai.configure(api_key=\"AIzaSyExample123...\")  # âœ… Real key format\n",
    "\n",
    "# Note: Get your real API key from https://aistudio.google.com/app/apikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06520f2b-9275-40f0-9939-4d3b6e22f078",
   "metadata": {
    "id": "06520f2b-9275-40f0-9939-4d3b6e22f078"
   },
   "outputs": [],
   "source": [
    "# ERROR #5: Empty or invalid API key\n",
    "# WRONG: Empty string or placeholder text\n",
    "# genai.configure(api_key=\"\")  # âŒ Empty key won't work\n",
    "# genai.configure(api_key=\"YOUR_API_KEY\")  # âŒ Placeholder text won't work\n",
    "\n",
    "# CORRECT: Use your actual API key\n",
    "# genai.configure(api_key=\"AIzaSyExample123...\")  # âœ… Real key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b7488ff-4c25-4365-bb2a-6a35beb530d2",
   "metadata": {
    "id": "5b7488ff-4c25-4365-bb2a-6a35beb530d2"
   },
   "outputs": [],
   "source": [
    "# ERROR #6: Wrong model names or missing variable assignment\n",
    "# WRONG: Incorrect model name\n",
    "# model = genai.GenerativeModel('gemini-flash')  # âŒ Missing version number\n",
    "\n",
    "# CORRECT: Use the full, correct model name\n",
    "model = genai.GenerativeModel('gemini-2.5-flash')  # âœ… Correct name\n",
    "\n",
    "# Also remember to assign to a variable (like 'model') so you can use it later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b662208f-02cf-4642-b93d-65e59d411518",
   "metadata": {
    "id": "b662208f-02cf-4642-b93d-65e59d411518"
   },
   "outputs": [],
   "source": [
    "# ERROR #7: Wrong method names\n",
    "# WRONG: Incorrect method name\n",
    "# response = model.generate('who are the top 5 cricketers in India')  # âŒ Wrong method\n",
    "\n",
    "# CORRECT: Use the full method name\n",
    "response = model.generate_content('who are the top 5 cricketers in India')  # âœ… Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0381b35e-57e7-428e-a8f7-2dbe6c62d295",
   "metadata": {
    "id": "0381b35e-57e7-428e-a8f7-2dbe6c62d295",
    "outputId": "0443d99c-6b37-4066-9c6b-eab1cc3b5d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response.text\n"
     ]
    }
   ],
   "source": [
    "# ERROR #8: Printing the wrong thing\n",
    "# WRONG: Printing the string \"response.text\" instead of the variable\n",
    "print(\"response.text\")  # âŒ This prints the literal text \"response.text\"\n",
    "\n",
    "# CORRECT: Print the actual response content (without quotes)\n",
    "# print(response.text)  # âœ… This prints the AI's actual response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3a743d9-2456-44a3-b3f7-a98aa1ecbd7e",
   "metadata": {
    "id": "f3a743d9-2456-44a3-b3f7-a98aa1ecbd7e"
   },
   "outputs": [],
   "source": [
    "# ðŸŽ“ MY LEARNING MINDSET - What I Tell Myself\n",
    "#\n",
    "# âœ… I'm okay with making mistakes - that's how I learn!\n",
    "# âœ… Each error teaches me something new about how code works\n",
    "# âœ… My goal is to learn from mistakes and not repeat them\n",
    "# âœ… I try new things and experiment - that's how I improve!\n",
    "#\n",
    "# ðŸš€ What I want to try next:\n",
    "# - Different prompts and questions\n",
    "# - Experiment with different model parameters  \n",
    "# - Build small projects using what I've learned\n",
    "#\n",
    "# I remind myself: Every expert was once a beginner like me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f470ba5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
